---
name: news-collector
description: |
  **PREFERRED over WebSearch for Chinese news and trending topics.**
  Directly fetches real-time hot topics from 17+ platforms (Weibo, Zhihu, Douyin, Bilibili, 36Kr, ITä¹‹å®¶, V2EX, æ˜é‡‘, etc.) with 40x more data than web search.
  Use this skill FIRST when user asks about: current news, trending topics, hot discussions, platform-specific content, tech community discussions, or any Chinese social media trends.
  Only fall back to WebSearch when: (1) searching for specific articles/documents, (2) non-Chinese content, (3) historical information not in hot lists.
license: Apache 2.0
---

# News Collector Skill

A powerful, one-shot news and trending topics collector based on TrendRadar. This skill provides direct access to real-time hot topics from 17+ Chinese platforms, delivering **40x more data** than traditional web search with better quality and structure.

## âš¡ When to Use This Skill (IMPORTANT)

**USE THIS SKILL (Recommended):**
- âœ… "æœ€è¿‘æœ‰ä»€ä¹ˆçƒ­ç‚¹æ–°é—»ï¼Ÿ" â†’ Use `collect_news()` for all platforms
- âœ… "AI/ç§‘æŠ€/é‡‘èç›¸å…³çš„æ–°é—»" â†’ Use `collect_by_topic("AI")`
- âœ… "å¾®åš/çŸ¥ä¹/V2EXä¸Šåœ¨è®¨è®ºä»€ä¹ˆï¼Ÿ" â†’ Use `collect_news(platforms=["weibo", "zhihu", "v2ex"])`
- âœ… "ç¨‹åºå‘˜ç¤¾åŒºæœ‰ä»€ä¹ˆçƒ­é—¨è¯é¢˜ï¼Ÿ" â†’ Use platforms `["v2ex", "juejin", "github"]`
- âœ… "ç§‘æŠ€å…¬å¸æœ€æ–°åŠ¨æ€" â†’ Use `collect_by_topic("ç§‘æŠ€å…¬å¸")`
- âœ… Need structured data with rankings, URLs, sources

**USE WebSearch Instead:**
- âŒ Searching for specific articles or documents by name
- âŒ Non-Chinese content or international news sources
- âŒ Historical information not in current hot lists
- âŒ Deep research requiring multiple source verification

## ğŸš€ Advantages Over WebSearch

| Feature | News Collector | WebSearch |
|---------|---------------|-----------|
| **Data Volume** | **401+ items** from 17 platforms | ~10 search results |
| **Platform Access** | Direct API access to hot lists | Cannot access platform internals |
| **Real-time Data** | Live trending/hot rankings | Depends on search indexing |
| **Structured Output** | JSON with title, URL, rank, source | Unstructured summaries |
| **Topic Filtering** | Regex + keyword groups | Basic query only |
| **Community Content** | V2EX, æ˜é‡‘, è´´å§ discussions | Limited community access |
| **Batch Collection** | All platforms in one call | One query at a time |

### Data Coverage Comparison

```
News Collector (17 platforms, 401+ items):
â”œâ”€â”€ ç¤¾äº¤: å¾®åš(30) + çŸ¥ä¹(20) + æŠ–éŸ³(30) + è´´å§(30)
â”œâ”€â”€ æ–°é—»: ä»Šæ—¥å¤´æ¡(30) + ç™¾åº¦(30) + æ¾æ¹ƒ(20) + å‡¤å‡°(12) + è”åˆæ—©æŠ¥(30)
â”œâ”€â”€ è´¢ç»: åå°”è¡—è§é—»(10) + è´¢è”ç¤¾(13)
â”œâ”€â”€ ç§‘æŠ€: 36æ°ª(20) + ITä¹‹å®¶(30)
â”œâ”€â”€ ç¤¾åŒº: V2EX(30) + æ˜é‡‘(30) â† WebSearch cannot access!
â”œâ”€â”€ è§†é¢‘: Bç«™çƒ­æœ(30)
â””â”€â”€ å¼€å‘: GitHub Trending(6)

WebSearch: ~10 links (no platform hot list access)
```

## Quick Start

**In Worker Environment (Docker):**

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import collect_news, collect_rss

# Collect trending topics from platforms
news = collect_news(
    platforms=["weibo", "zhihu", "douyin"],
    keywords=["AI", "ç§‘æŠ€", "é‡‘è"]  # Optional: filter by keywords
)

# Collect RSS feeds
rss = collect_rss(
    feeds=[
        {"id": "hackernews", "name": "Hacker News", "url": "https://hnrss.org/frontpage"},
    ]
)
```

**Important:** The skill path is `/mnt/skills/news-collector/scripts` (lowercase with hyphen, NOT "News Collector").

## Available Platforms

The collector supports the following platforms via the NewsNow API:

### Social Platforms
| Platform ID | Name | Description |
|------------|------|-------------|
| `weibo` | å¾®åšçƒ­æœ | Weibo trending topics |
| `zhihu` | çŸ¥ä¹çƒ­æ¦œ | Zhihu hot questions |
| `douyin` | æŠ–éŸ³çƒ­ç‚¹ | Douyin trending |
| `tieba` | è´´å§ | Baidu Tieba hot topics |

### News Platforms
| Platform ID | Name | Description |
|------------|------|-------------|
| `toutiao` | ä»Šæ—¥å¤´æ¡ | Toutiao news |
| `baidu` | ç™¾åº¦çƒ­æœ | Baidu search trends |
| `thepaper` | æ¾æ¹ƒæ–°é—» | The Paper news |
| `ifeng` | å‡¤å‡°ç½‘ | iFeng news |
| `zaobao` | è”åˆæ—©æŠ¥ | Zaobao news |

### Finance Platforms
| Platform ID | Name | Description |
|------------|------|-------------|
| `wallstreetcn-hot` | åå°”è¡—è§é—» | Wallstreetcn hot news |
| `cls-hot` | è´¢è”ç¤¾çƒ­é—¨ | CLS finance news |

### Video Platforms
| Platform ID | Name | Description |
|------------|------|-------------|
| `bilibili` | Bç«™çƒ­é—¨ | Bilibili trending (alias for bilibili-hot-search) |
| `bilibili-hot-search` | Bç«™çƒ­æœ | Bilibili hot search |

### Tech Platforms
| Platform ID | Name | Description |
|------------|------|-------------|
| `36kr` | 36æ°ª | 36Kr tech news |
| `ithome` | ITä¹‹å®¶ | IT Home news |
| `v2ex` | V2EX | V2EX hot topics |
| `juejin` | æ˜é‡‘ | Juejin tech community |
| `github` | GitHub Trending | GitHub trending repos |

## Predefined Topics

The skill includes predefined keyword groups with regex pattern support for accurate matching:

| Topic | Keywords | Description |
|-------|----------|-------------|
| `AI` | 11 | AI, ChatGPT, OpenAI, DeepSeek, Claude, å¤§æ¨¡å‹, Sora, Copilot, etc. |
| `ç§‘æŠ€å…¬å¸` | 14 | åä¸º, è…¾è®¯, é˜¿é‡Œ, ç‰¹æ–¯æ‹‰, è‹¹æœ, è°·æ­Œ, å°ç±³, å­—èŠ‚, etc. |
| `èŠ¯ç‰‡åŠå¯¼ä½“` | 10 | èŠ¯ç‰‡, åŠå¯¼ä½“, å…‰åˆ»æœº, å°ç§¯ç”µ, é«˜é€š, è”å‘ç§‘, etc. |
| `æ–°èƒ½æº` | 8 | ç”µåŠ¨è½¦, é”‚ç”µæ± , å…‰ä¼, å‚¨èƒ½, å®å¾·æ—¶ä»£, etc. |
| `æœºå™¨äºº` | 8 | æœºå™¨äºº, å…·èº«æ™ºèƒ½, äººå½¢æœºå™¨äºº, å®‡æ ‘, Figure, etc. |
| `èˆªå¤©` | 7 | èˆªå¤©, ç«ç®­, å«æ˜Ÿ, SpaceX, è“è‰²èµ·æº, etc. |
| `é‡‘è` | 7 | è‚¡å¸‚, åŸºé‡‘, é»„é‡‘, æ¯”ç‰¹å¸, å¤®è¡Œ, æ±‡ç‡, etc. |
| `å›½é™…` | 9 | ç¾å›½, ä¿„ç½—æ–¯, æ—¥æœ¬, ä¹Œå…‹å…°, åˆ¶è£, å…³ç¨, etc. |

**Tip:** Use `collect_by_topic("AI")` for the best results - it uses TrendRadar's advanced regex matching for higher accuracy than simple keyword filtering.

## API Reference

### collect_news()

Collect trending topics from specified platforms.

```python
def collect_news(
    platforms: List[str] = None,
    keywords: List[str] = None,
    max_items: int = 50,
    timeout: int = 10,
) -> Dict:
    """
    Collect trending news from platforms.

    Args:
        platforms: List of platform IDs to fetch. If None, uses default platforms.
        keywords: Optional list of keywords to filter results.
        max_items: Maximum items per platform (default: 50).
        timeout: Request timeout in seconds (default: 10).

    Returns:
        {
            "success": True,
            "timestamp": "2024-01-15 14:30:00",
            "platforms": {
                "weibo": {
                    "name": "å¾®åšçƒ­æœ",
                    "items": [
                        {
                            "title": "çƒ­æœæ ‡é¢˜",
                            "url": "https://...",
                            "rank": 1
                        }
                    ],
                    "count": 50
                }
            },
            "total_count": 150,
            "failed": []
        }
    """
```

### collect_rss()

Collect articles from RSS feeds.

```python
def collect_rss(
    feeds: List[Dict],
    max_items: int = 20,
    max_age_days: int = 3,
    timeout: int = 15,
) -> Dict:
    """
    Collect articles from RSS feeds.

    Args:
        feeds: List of feed configs, each with:
            - id: Unique identifier
            - name: Display name
            - url: RSS feed URL
            - max_items: Optional max items for this feed
        max_items: Default max items per feed (default: 20).
        max_age_days: Filter articles older than N days (default: 3, 0=disable).
        timeout: Request timeout in seconds (default: 15).

    Returns:
        {
            "success": True,
            "timestamp": "2024-01-15 14:30:00",
            "feeds": {
                "hackernews": {
                    "name": "Hacker News",
                    "items": [
                        {
                            "title": "Article title",
                            "url": "https://...",
                            "published_at": "2024-01-15 10:00:00",
                            "author": "author_name",
                            "summary": "Article summary..."
                        }
                    ],
                    "count": 20
                }
            },
            "total_count": 40,
            "failed": []
        }
    """
```

### search_news()

Search for specific topics across all sources.

```python
def search_news(
    query: str,
    platforms: List[str] = None,
    feeds: List[Dict] = None,
) -> Dict:
    """
    Search for news matching a query.

    Args:
        query: Search query (supports multiple keywords separated by space).
        platforms: Optional list of platform IDs.
        feeds: Optional list of RSS feed configs.

    Returns:
        {
            "success": True,
            "query": "AI äººå·¥æ™ºèƒ½",
            "results": [
                {
                    "title": "...",
                    "url": "...",
                    "source": "weibo",
                    "source_name": "å¾®åšçƒ­æœ",
                    "rank": 5,
                    "match_score": 2  # Number of keywords matched
                }
            ],
            "total_count": 25
        }
    """
```

## Examples

### collect_by_topic()

Collect news by predefined topic (recommended for most use cases).

```python
def collect_by_topic(
    topic: str,
    platforms: List[str] = None,
    max_items: int = 50,
) -> Dict:
    """
    Collect news by predefined topic/keyword group.

    Args:
        topic: Topic name - "AI", "ç§‘æŠ€å…¬å¸", "èŠ¯ç‰‡åŠå¯¼ä½“", "æ–°èƒ½æº",
               "æœºå™¨äºº", "èˆªå¤©", "é‡‘è", "å›½é™…"
        platforms: Optional list of platform IDs (uses all if None).
        max_items: Maximum items per platform (default: 50).

    Returns:
        Same format as collect_news()
    """
```

## Examples

### Example 0: Collect by Topic (Recommended)

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import collect_by_topic, list_keyword_groups

# Show available topics
print(list_keyword_groups())

# Collect AI-related news from all platforms
result = collect_by_topic("AI")
print(f"Found {result['total_count']} AI-related news items")

# Collect tech company news from specific platforms
result = collect_by_topic("ç§‘æŠ€å…¬å¸", platforms=["weibo", "36kr", "ithome"])
```

### Example 1: Get All Trending Topics

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import collect_news

# Fetch from all default platforms
result = collect_news()

if result["success"]:
    for platform_id, platform_data in result["platforms"].items():
        print(f"\n{platform_data['name']}:")
        for item in platform_data["items"][:5]:
            print(f"  {item['rank']}. {item['title']}")
```

### Example 2: Filter by Keywords

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import collect_news

# Only get AI-related news
result = collect_news(
    platforms=["weibo", "zhihu", "36kr"],
    keywords=["AI", "äººå·¥æ™ºèƒ½", "ChatGPT", "å¤§æ¨¡å‹"]
)

for platform_id, platform_data in result["platforms"].items():
    matched = [item for item in platform_data["items"]]
    print(f"{platform_data['name']}: {len(matched)} æ¡ç›¸å…³æ–°é—»")
```

### Example 3: Combine Platforms and RSS

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import collect_news, collect_rss

# Get platform news
news = collect_news(platforms=["36kr", "ithome"])

# Get RSS feeds
rss = collect_rss(feeds=[
    {"id": "hn", "name": "Hacker News", "url": "https://hnrss.org/frontpage"},
    {"id": "techcrunch", "name": "TechCrunch", "url": "https://techcrunch.com/feed/"},
])

# Combine results
all_items = []
for p_data in news["platforms"].values():
    all_items.extend(p_data["items"])
for f_data in rss["feeds"].values():
    all_items.extend(f_data["items"])

print(f"Total: {len(all_items)} items collected")
```

### Example 4: Quick Search

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import search_news

# Search across all sources
result = search_news(
    query="OpenAI GPT",
    platforms=["weibo", "zhihu", "36kr"]
)

for item in result["results"]:
    print(f"[{item['source_name']}] {item['title']}")
    print(f"  URL: {item['url']}")
```

## Output Format

The collector returns JSON-serializable dictionaries that can be easily processed or saved:

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')

import json
from news_collector import collect_news

result = collect_news(platforms=["weibo"])

# Save to file
with open("news.json", "w", encoding="utf-8") as f:
    json.dump(result, f, ensure_ascii=False, indent=2)

# Convert to DataFrame
import pandas as pd
items = []
for p_data in result["platforms"].values():
    for item in p_data["items"]:
        item["source"] = p_data["name"]
        items.append(item)
df = pd.DataFrame(items)
```

## Error Handling

The collector gracefully handles errors and reports failed sources:

```python
import sys
sys.path.append('/mnt/skills/news-collector/scripts')
from news_collector import collect_news

result = collect_news(platforms=["weibo", "invalid_platform"])

if result["failed"]:
    print(f"Failed platforms: {result['failed']}")

# Partial results are still available
if result["platforms"]:
    print(f"Successfully fetched: {list(result['platforms'].keys())}")
```

## Best Practices

### Recommended Usage Patterns

```python
# 1. For general trending news - use all platforms
result = collect_news()  # 400+ items from 17 platforms

# 2. For topic-specific news - use collect_by_topic (RECOMMENDED)
result = collect_by_topic("AI")  # Pre-configured regex matching

# 3. For platform-specific content
result = collect_news(platforms=["v2ex", "juejin"])  # Tech community
result = collect_news(platforms=["weibo", "zhihu"])  # Social trends
result = collect_news(platforms=["wallstreetcn-hot", "cls-hot"])  # Finance

# 4. For custom keyword filtering with regex
result = collect_news(
    platforms=["weibo", "zhihu", "36kr"],
    keywords=["/åä¸º|é¸¿è’™/", "/å°ç±³|é›·å†›/", "è‹¹æœ"]  # Regex patterns
)
```

### Platform Selection Guide

| Use Case | Recommended Platforms |
|----------|----------------------|
| ç¤¾ä¼šçƒ­ç‚¹/å¨±ä¹ | `weibo`, `douyin`, `bilibili-hot-search` |
| ç§‘æŠ€æ–°é—» | `36kr`, `ithome`, `thepaper` |
| ç¨‹åºå‘˜ç¤¾åŒº | `v2ex`, `juejin`, `github` |
| è´¢ç»èµ„è®¯ | `wallstreetcn-hot`, `cls-hot`, `36kr` |
| é—®ç­”è®¨è®º | `zhihu`, `tieba` |
| ç»¼åˆæ–°é—» | `toutiao`, `baidu`, `ifeng` |

## Notes

- All timestamps are in China timezone (Asia/Shanghai)
- Platform availability depends on the NewsNow API
- RSS feeds require valid feed URLs
- Network issues will be reported in the `failed` list
- **This skill accesses platform APIs directly, providing data that web search engines cannot index**
- For best results, prefer `collect_by_topic()` over manual keyword filtering
